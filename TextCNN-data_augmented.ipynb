{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6df03b8b-0237-4433-a7f4-5b761aad4472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Full data shape: (16386, 8)\n",
      "Train shape (80%): (13108, 8)\n",
      "Dev/Test shape (20%): (3278, 8)\n",
      "Subset Train shape: (13108, 8)\n",
      "Full Dev/Test shape: (3278, 8)\n",
      "Vocab size: 10205\n",
      "Encoding train texts...\n",
      "Encoding dev texts...\n",
      "Train batches: 205\n",
      "Dev batches: 26\n",
      "TextCNN(\n",
      "  (embedding): Embedding(10205, 100, padding_idx=0)\n",
      "  (convs): ModuleList(\n",
      "    (0): Conv1d(100, 100, kernel_size=(3,), stride=(1,))\n",
      "    (1): Conv1d(100, 100, kernel_size=(4,), stride=(1,))\n",
      "    (2): Conv1d(100, 100, kernel_size=(5,), stride=(1,))\n",
      "  )\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=300, out_features=7, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8154718a05e64e148ec0a9c857517c31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/15:   0%|          | 0/205 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 done | train_loss=0.3078 | dev_micro=0.7185 | dev_macro=0.6153\n",
      "  → New best model (dev Micro-F1=0.7185)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49bc6b0c122049f0aa6fe26467eb0d77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/15:   0%|          | 0/205 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 done | train_loss=0.2005 | dev_micro=0.7723 | dev_macro=0.6813\n",
      "  → New best model (dev Micro-F1=0.7723)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b96381c1410242baa15b9572fb5493ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/15:   0%|          | 0/205 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 done | train_loss=0.1705 | dev_micro=0.7881 | dev_macro=0.6939\n",
      "  → New best model (dev Micro-F1=0.7881)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ebac85d0b7140bf9f4b9b80583fce21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/15:   0%|          | 0/205 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4 done | train_loss=0.1522 | dev_micro=0.7951 | dev_macro=0.7180\n",
      "  → New best model (dev Micro-F1=0.7951)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cf0b2ccdb7141f69d841c690b276048",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/15:   0%|          | 0/205 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5 done | train_loss=0.1388 | dev_micro=0.7986 | dev_macro=0.7317\n",
      "  → New best model (dev Micro-F1=0.7986)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04bcf461afc54bacaa04c9b3f6b7f828",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/15:   0%|          | 0/205 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6 done | train_loss=0.1275 | dev_micro=0.8042 | dev_macro=0.7217\n",
      "  → New best model (dev Micro-F1=0.8042)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ae5ed6f7982433baead80c2055a4869",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/15:   0%|          | 0/205 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7 done | train_loss=0.1189 | dev_micro=0.7998 | dev_macro=0.7172\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01e36f8202ea4758bc2672bd53f129e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/15:   0%|          | 0/205 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8 done | train_loss=0.1098 | dev_micro=0.8066 | dev_macro=0.7409\n",
      "  → New best model (dev Micro-F1=0.8066)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c657f8bdd2c54a4fa3722931e5ec33fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/15:   0%|          | 0/205 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9 done | train_loss=0.1006 | dev_micro=0.8054 | dev_macro=0.7295\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5029f34504d54f0d8641cd9e97d36504",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/15:   0%|          | 0/205 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10 done | train_loss=0.0943 | dev_micro=0.8067 | dev_macro=0.7369\n",
      "  → New best model (dev Micro-F1=0.8067)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5c8274913584096954354855fed6522",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 11/15:   0%|          | 0/205 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11 done | train_loss=0.0875 | dev_micro=0.8031 | dev_macro=0.7266\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c81a87d088674109a3376b0caf4cd1af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 12/15:   0%|          | 0/205 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12 done | train_loss=0.0814 | dev_micro=0.8028 | dev_macro=0.7356\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "848ee486d1054bdaa1d5d52fb8e48756",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 13/15:   0%|          | 0/205 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13 done | train_loss=0.0761 | dev_micro=0.8018 | dev_macro=0.7327\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ea6737da1ac4b9fadef0e7793ec7cf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 14/15:   0%|          | 0/205 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14 done | train_loss=0.0705 | dev_micro=0.7989 | dev_macro=0.7331\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fc5e43ce31b44e8956eac3b8f33eb8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 15/15:   0%|          | 0/205 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15 done | train_loss=0.0670 | dev_micro=0.7977 | dev_macro=0.7311\n",
      "\n",
      "Best model restored!\n",
      "\n",
      "Searching best global threshold on FULL dev...\n",
      "t=0.10 → Dev Micro-F1 = 0.7609\n",
      "t=0.15 → Dev Micro-F1 = 0.7713\n",
      "t=0.20 → Dev Micro-F1 = 0.7791\n",
      "t=0.25 → Dev Micro-F1 = 0.7824\n",
      "t=0.30 → Dev Micro-F1 = 0.7869\n",
      "t=0.35 → Dev Micro-F1 = 0.7883\n",
      "t=0.40 → Dev Micro-F1 = 0.7907\n",
      "t=0.45 → Dev Micro-F1 = 0.7948\n",
      "t=0.50 → Dev Micro-F1 = 0.7977\n",
      "t=0.55 → Dev Micro-F1 = 0.7994\n",
      "t=0.60 → Dev Micro-F1 = 0.7993\n",
      "t=0.65 → Dev Micro-F1 = 0.7991\n",
      "t=0.70 → Dev Micro-F1 = 0.7971\n",
      "t=0.75 → Dev Micro-F1 = 0.7946\n",
      "t=0.80 → Dev Micro-F1 = 0.7904\n",
      "t=0.85 → Dev Micro-F1 = 0.7843\n",
      "\n",
      "BEST threshold: 0.55\n",
      "BEST dev Micro-F1: 0.7994\n",
      "\n",
      "========================\n",
      " FINAL DEV PERFORMANCE  \n",
      "========================\n",
      "Micro-F1 (t=0.55): 0.80\n",
      "Macro-F1 (t=0.55): 0.73\n",
      "\n",
      "Admiration F1: 0.78\n",
      "Amusement F1: 0.90\n",
      "Gratitude F1: 0.94\n",
      "Love F1: 0.88\n",
      "Pride F1: 0.47\n",
      "Relief F1: 0.23\n",
      "Remorse F1: 0.90\n",
      "\n",
      "Saved dev_predictions_textcnn.csv!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim import Adam\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ============================================================\n",
    "# 1. DEVICE & SEED\n",
    "# ============================================================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# ============================================================\n",
    "# 2. LOAD DATA + SPLIT 80:20\n",
    "# ============================================================\n",
    "df = pd.read_csv(\"traindata_final_fixed_rulebased.csv\")\n",
    "\n",
    "label_cols = [\"admiration\",\"amusement\",\"gratitude\",\"love\",\"pride\",\"relief\",\"remorse\"]\n",
    "\n",
    "print(\"Full data shape:\", df.shape)\n",
    "\n",
    "train_df, dev_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Train shape (80%):\", train_df.shape)\n",
    "print(\"Dev/Test shape (20%):\", dev_df.shape)\n",
    "\n",
    "# Kalau mau subset train, ubah N_TRAIN. Sekarang pakai semua data train.\n",
    "N_TRAIN = len(train_df)\n",
    "\n",
    "train_df_small = train_df.sample(\n",
    "    n=min(N_TRAIN, len(train_df)),\n",
    "    random_state=42\n",
    ").reset_index(drop=True)\n",
    "\n",
    "dev_df_full = dev_df.reset_index(drop=True)\n",
    "\n",
    "print(\"Subset Train shape:\", train_df_small.shape)\n",
    "print(\"Full Dev/Test shape:\", dev_df_full.shape)\n",
    "\n",
    "X_train_texts = train_df_small[\"text\"].astype(str).tolist()\n",
    "X_dev_texts   = dev_df_full[\"text\"].astype(str).tolist()\n",
    "\n",
    "y_train = train_df_small[label_cols].values.astype(\"float32\")\n",
    "y_dev   = dev_df_full[label_cols].values.astype(\"float32\")\n",
    "\n",
    "# ============================================================\n",
    "# 3. TOKENISASI SEDERHANA + VOCAB\n",
    "# ============================================================\n",
    "from collections import Counter\n",
    "\n",
    "def tokenize(text):\n",
    "    text = text.lower()\n",
    "    # ambil token alfanumerik sederhana\n",
    "    tokens = re.findall(r\"\\b\\w+\\b\", text)\n",
    "    return tokens\n",
    "\n",
    "# Bangun vocab dari TRAIN SAJA (jangan dari dev)\n",
    "counter = Counter()\n",
    "for txt in X_train_texts:\n",
    "    counter.update(tokenize(txt))\n",
    "\n",
    "MAX_VOCAB_SIZE = 30000  # bisa diubah\n",
    "most_common = counter.most_common(MAX_VOCAB_SIZE)\n",
    "\n",
    "# 0 = PAD, 1 = UNK\n",
    "word2idx = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "for i, (word, freq) in enumerate(most_common, start=2):\n",
    "    word2idx[word] = i\n",
    "\n",
    "idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "\n",
    "vocab_size = len(word2idx)\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "\n",
    "def numericalize(tokens, word2idx):\n",
    "    return [word2idx.get(tok, word2idx[\"<UNK>\"]) for tok in tokens]\n",
    "\n",
    "MAX_LEN = 50  # panjang sequence, bisa diubah\n",
    "\n",
    "def encode_texts(texts, word2idx, max_len):\n",
    "    all_ids = []\n",
    "    for txt in texts:\n",
    "        toks = tokenize(txt)\n",
    "        ids = numericalize(toks, word2idx)\n",
    "        # pad / truncate\n",
    "        if len(ids) < max_len:\n",
    "            ids = ids + [word2idx[\"<PAD>\"]] * (max_len - len(ids))\n",
    "        else:\n",
    "            ids = ids[:max_len]\n",
    "        all_ids.append(ids)\n",
    "    return np.array(all_ids, dtype=\"int64\")\n",
    "\n",
    "print(\"Encoding train texts...\")\n",
    "X_train_ids = encode_texts(X_train_texts, word2idx, MAX_LEN)\n",
    "print(\"Encoding dev texts...\")\n",
    "X_dev_ids   = encode_texts(X_dev_texts,   word2idx, MAX_LEN)\n",
    "\n",
    "# ============================================================\n",
    "# 4. DATASET & DATALOADER\n",
    "# ============================================================\n",
    "X_train_tensor = torch.tensor(X_train_ids, dtype=torch.long)\n",
    "y_train_tensor = torch.tensor(y_train,     dtype=torch.float32)\n",
    "\n",
    "X_dev_tensor   = torch.tensor(X_dev_ids,   dtype=torch.long)\n",
    "y_dev_tensor   = torch.tensor(y_dev,       dtype=torch.float32)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "dev_dataset   = TensorDataset(X_dev_tensor,   y_dev_tensor)\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "dev_loader   = DataLoader(dev_dataset,   batch_size=128, shuffle=False)\n",
    "\n",
    "print(\"Train batches:\", len(train_loader))\n",
    "print(\"Dev batches:\", len(dev_loader))\n",
    "\n",
    "# ============================================================\n",
    "# 5. MODEL TEXTCNN\n",
    "# ============================================================\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 embed_dim,\n",
    "                 num_labels,\n",
    "                 kernel_sizes=(3, 4, 5),\n",
    "                 num_filters=100,\n",
    "                 dropout=0.5,\n",
    "                 pad_idx=0):\n",
    "        super(TextCNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=embed_dim,\n",
    "            padding_idx=pad_idx\n",
    "        )\n",
    "\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(\n",
    "                in_channels=embed_dim,\n",
    "                out_channels=num_filters,\n",
    "                kernel_size=k\n",
    "            )\n",
    "            for k in kernel_sizes\n",
    "        ])\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(num_filters * len(kernel_sizes), num_labels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch, seq_len]\n",
    "        embedded = self.embedding(x)          # [B, L, E]\n",
    "        embedded = embedded.transpose(1, 2)   # [B, E, L]\n",
    "\n",
    "        conv_outs = [torch.relu(conv(embedded)) for conv in self.convs]\n",
    "        # conv_out: [B, num_filters, L_out]\n",
    "\n",
    "        pooled = [torch.max(co, dim=2)[0] for co in conv_outs]\n",
    "        # pooled: [B, num_filters]\n",
    "\n",
    "        cat = torch.cat(pooled, dim=1)        # [B, num_filters * len(kernel_sizes)]\n",
    "        cat = self.dropout(cat)\n",
    "        logits = self.fc(cat)                 # [B, num_labels]\n",
    "        return logits\n",
    "\n",
    "num_labels = len(label_cols)\n",
    "\n",
    "embed_dim   = 100\n",
    "kernel_sizes = (3, 4, 5)\n",
    "num_filters = 100\n",
    "dropout     = 0.5\n",
    "\n",
    "model = TextCNN(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=embed_dim,\n",
    "    num_labels=num_labels,\n",
    "    kernel_sizes=kernel_sizes,\n",
    "    num_filters=num_filters,\n",
    "    dropout=dropout,\n",
    "    pad_idx=word2idx[\"<PAD>\"]\n",
    ").to(device)\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "print(model)\n",
    "\n",
    "# ============================================================\n",
    "# 6. EVALUATION FUNCTION\n",
    "# ============================================================\n",
    "def evaluate(threshold=0.5):\n",
    "    model.eval()\n",
    "    all_logits = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for ids, labels in dev_loader:\n",
    "            ids    = ids.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            logits = model(ids)\n",
    "\n",
    "            all_logits.append(logits.cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "    all_logits = np.concatenate(all_logits, axis=0)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "    probs = 1 / (1 + np.exp(-all_logits))  # sigmoid\n",
    "    preds = (probs >= threshold).astype(int)\n",
    "\n",
    "    micro = f1_score(all_labels, preds, average=\"micro\")\n",
    "    macro = f1_score(all_labels, preds, average=\"macro\")\n",
    "\n",
    "    return micro, macro, probs, preds, all_labels\n",
    "\n",
    "# ============================================================\n",
    "# 7. TRAINING LOOP + SIMPAN BEST MODEL (BERDASARKAN MICRO-F1 DEV)\n",
    "# ============================================================\n",
    "epochs = 15\n",
    "\n",
    "best_micro = -1.0\n",
    "best_state = None\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{epochs}\")\n",
    "    for ids, labels in pbar:\n",
    "        ids    = ids.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(ids)\n",
    "        loss   = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * ids.size(0)\n",
    "        pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "    train_loss = total_loss / len(train_loader.dataset)\n",
    "    dev_micro, dev_macro, _, _, _ = evaluate(threshold=0.5)\n",
    "\n",
    "    print(f\"\\nEpoch {epoch} done | train_loss={train_loss:.4f} | \"\n",
    "          f\"dev_micro={dev_micro:.4f} | dev_macro={dev_macro:.4f}\")\n",
    "\n",
    "    if dev_micro > best_micro:\n",
    "        best_micro = dev_micro\n",
    "        best_state = model.state_dict().copy()\n",
    "        print(f\"  → New best model (dev Micro-F1={best_micro:.4f})\")\n",
    "\n",
    "# restore best model\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n",
    "    print(\"\\nBest model restored!\")\n",
    "\n",
    "# ============================================================\n",
    "# 8. CARI THRESHOLD GLOBAL TERBAIK DI FULL DEV\n",
    "# ============================================================\n",
    "print(\"\\nSearching best global threshold on FULL dev...\")\n",
    "\n",
    "best_t = 0.5\n",
    "best_t_micro = -1.0\n",
    "\n",
    "for t in np.arange(0.1, 0.9, 0.05):\n",
    "    micro_t, _, _, _, _ = evaluate(threshold=t)\n",
    "    print(f\"t={t:.2f} → Dev Micro-F1 = {micro_t:.4f}\")\n",
    "    if micro_t > best_t_micro:\n",
    "        best_t_micro = micro_t\n",
    "        best_t = t\n",
    "\n",
    "print(f\"\\nBEST threshold: {best_t:.2f}\")\n",
    "print(f\"BEST dev Micro-F1: {best_t_micro:.4f}\")\n",
    "\n",
    "# ============================================================\n",
    "# 9. FINAL EVAL + PER-LABEL F1 DI FULL DEV\n",
    "# ============================================================\n",
    "final_micro, final_macro, final_probs, final_preds, final_true = evaluate(threshold=best_t)\n",
    "\n",
    "print(\"\\n========================\")\n",
    "print(\" FINAL DEV PERFORMANCE  \")\n",
    "print(\"========================\")\n",
    "print(f\"Micro-F1 (t={best_t:.2f}): {final_micro:.2f}\")\n",
    "print(f\"Macro-F1 (t={best_t:.2f}): {final_macro:.2f}\\n\")\n",
    "\n",
    "for i, col in enumerate(label_cols):\n",
    "    f1 = f1_score(final_true[:, i], final_preds[:, i])\n",
    "    print(f\"{col.capitalize()} F1: {f1:.2f}\")\n",
    "\n",
    "# ============================================================\n",
    "# 10. SAVE dev_predictions_textcnn.csv\n",
    "# ============================================================\n",
    "output_df = dev_df_full.copy()\n",
    "for i, col in enumerate(label_cols):\n",
    "    output_df[col] = final_preds[:, i]\n",
    "\n",
    "output_df.to_csv(\"dev_predictions_textcnn.csv\", index=False)\n",
    "print(\"\\nSaved dev_predictions_textcnn.csv!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecf557a-c54d-40b9-a9ab-359411857851",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python for Language Modelling",
   "language": "python",
   "name": "lm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
