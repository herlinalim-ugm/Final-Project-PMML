{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6df03b8b-0237-4433-a7f4-5b761aad4472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Full data shape: (16386, 8)\n",
      "Train shape : (25196, 8)\n",
      "Dev/Test shape : (3149, 8)\n",
      "Subset Train shape: (25196, 8)\n",
      "Full Dev/Test shape: (3149, 8)\n",
      "Vocab size: 20278\n",
      "Encoding train texts...\n",
      "Encoding dev texts...\n",
      "Train batches: 394\n",
      "Dev batches: 25\n",
      "TextCNN(\n",
      "  (embedding): Embedding(20278, 100, padding_idx=0)\n",
      "  (convs): ModuleList(\n",
      "    (0): Conv1d(100, 100, kernel_size=(3,), stride=(1,))\n",
      "    (1): Conv1d(100, 100, kernel_size=(4,), stride=(1,))\n",
      "    (2): Conv1d(100, 100, kernel_size=(5,), stride=(1,))\n",
      "  )\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=300, out_features=7, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d52d0cd1386481c80f5de521a8d5234",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/15:   0%|          | 0/394 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 done | train_loss=0.1752 | dev_micro=0.6986 | dev_macro=0.5085\n",
      "  → New best model (dev Micro-F1=0.6986)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3d6e22b3f6a49f2b0925308b8e59e4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/15:   0%|          | 0/394 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 done | train_loss=0.1156 | dev_micro=0.7707 | dev_macro=0.5622\n",
      "  → New best model (dev Micro-F1=0.7707)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9805e50e9b4547588fcce8f5c1c9c2b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/15:   0%|          | 0/394 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 done | train_loss=0.0985 | dev_micro=0.7725 | dev_macro=0.5743\n",
      "  → New best model (dev Micro-F1=0.7725)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4b06c376f9b4511b72b969fe3882486",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/15:   0%|          | 0/394 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4 done | train_loss=0.0878 | dev_micro=0.7896 | dev_macro=0.5927\n",
      "  → New best model (dev Micro-F1=0.7896)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "394dfafc8dc6497d90ed27c468850d88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/15:   0%|          | 0/394 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5 done | train_loss=0.0791 | dev_micro=0.7906 | dev_macro=0.5896\n",
      "  → New best model (dev Micro-F1=0.7906)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d29ba65781343328d8ae67272b7f67c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/15:   0%|          | 0/394 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6 done | train_loss=0.0716 | dev_micro=0.8020 | dev_macro=0.5955\n",
      "  → New best model (dev Micro-F1=0.8020)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1439325a4ac4cd697509bd073ce469c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/15:   0%|          | 0/394 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7 done | train_loss=0.0645 | dev_micro=0.7984 | dev_macro=0.6269\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3773132b413247f2b28d7c8c27fbd93c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/15:   0%|          | 0/394 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8 done | train_loss=0.0577 | dev_micro=0.7966 | dev_macro=0.6357\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c67aef89e0604360b24859b2cebba54c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/15:   0%|          | 0/394 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9 done | train_loss=0.0527 | dev_micro=0.7858 | dev_macro=0.6134\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94ddd896aae24fffa68b4d4ccabd05c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/15:   0%|          | 0/394 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10 done | train_loss=0.0473 | dev_micro=0.7747 | dev_macro=0.6091\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee841adb2d114e8284d9cb3530dedb6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 11/15:   0%|          | 0/394 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11 done | train_loss=0.0432 | dev_micro=0.7893 | dev_macro=0.6142\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cdea1b5f5f9421c8333f914e23fd2de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 12/15:   0%|          | 0/394 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12 done | train_loss=0.0377 | dev_micro=0.7889 | dev_macro=0.6037\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ba6f195c71b4928a9a04ce9cb70f2ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 13/15:   0%|          | 0/394 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13 done | train_loss=0.0353 | dev_micro=0.7838 | dev_macro=0.6179\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e16faf3fdf1c44d9be717bb1eb9592b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 14/15:   0%|          | 0/394 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14 done | train_loss=0.0329 | dev_micro=0.7816 | dev_macro=0.5976\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6dbc3b228b749ec955a18025f63d437",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 15/15:   0%|          | 0/394 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15 done | train_loss=0.0303 | dev_micro=0.7742 | dev_macro=0.5942\n",
      "\n",
      "Best model restored!\n",
      "\n",
      "Searching best global threshold on FULL dev...\n",
      "t=0.10 → Dev Micro-F1 = 0.7776\n",
      "t=0.15 → Dev Micro-F1 = 0.7873\n",
      "t=0.20 → Dev Micro-F1 = 0.7917\n",
      "t=0.25 → Dev Micro-F1 = 0.7895\n",
      "t=0.30 → Dev Micro-F1 = 0.7858\n",
      "t=0.35 → Dev Micro-F1 = 0.7841\n",
      "t=0.40 → Dev Micro-F1 = 0.7808\n",
      "t=0.45 → Dev Micro-F1 = 0.7808\n",
      "t=0.50 → Dev Micro-F1 = 0.7742\n",
      "t=0.55 → Dev Micro-F1 = 0.7719\n",
      "t=0.60 → Dev Micro-F1 = 0.7692\n",
      "t=0.65 → Dev Micro-F1 = 0.7645\n",
      "t=0.70 → Dev Micro-F1 = 0.7604\n",
      "t=0.75 → Dev Micro-F1 = 0.7583\n",
      "t=0.80 → Dev Micro-F1 = 0.7519\n",
      "t=0.85 → Dev Micro-F1 = 0.7424\n",
      "\n",
      "BEST threshold: 0.20\n",
      "BEST dev Micro-F1: 0.7917\n",
      "\n",
      "========================\n",
      " FINAL DEV PERFORMANCE  \n",
      "========================\n",
      "Micro-F1 (t=0.20): 0.79\n",
      "Macro-F1 (t=0.20): 0.64\n",
      "\n",
      "Admiration F1: 0.72\n",
      "Amusement F1: 0.79\n",
      "Gratitude F1: 0.92\n",
      "Love F1: 0.80\n",
      "Pride F1: 0.22\n",
      "Relief F1: 0.17\n",
      "Remorse F1: 0.82\n",
      "\n",
      "Saved dev_predictions_textcnn.csv!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim import Adam\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ============================================================\n",
    "# 1. DEVICE & SEED\n",
    "# ============================================================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# ============================================================\n",
    "# 2. LOAD DATA + SPLIT 80:20\n",
    "# ============================================================\n",
    "df = pd.read_csv(\"traindata_final_fixed_rulebased.csv\")\n",
    "\n",
    "label_cols = [\"admiration\",\"amusement\",\"gratitude\",\"love\",\"pride\",\"relief\",\"remorse\"]\n",
    "\n",
    "print(\"Full data shape:\", df.shape)\n",
    "'''\n",
    "train_df, dev_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "'''\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "dev_df   = pd.read_csv(\"dev.csv\")\n",
    "\n",
    "print(\"Train shape :\", train_df.shape)\n",
    "print(\"Dev/Test shape :\", dev_df.shape)\n",
    "\n",
    "# Kalau mau subset train, ubah N_TRAIN. Sekarang pakai semua data train.\n",
    "N_TRAIN = len(train_df)\n",
    "\n",
    "train_df_small = train_df.sample(\n",
    "    n=min(N_TRAIN, len(train_df)),\n",
    "    random_state=42\n",
    ").reset_index(drop=True)\n",
    "\n",
    "dev_df_full = dev_df.reset_index(drop=True)\n",
    "\n",
    "print(\"Subset Train shape:\", train_df_small.shape)\n",
    "print(\"Full Dev/Test shape:\", dev_df_full.shape)\n",
    "\n",
    "X_train_texts = train_df_small[\"text\"].astype(str).tolist()\n",
    "X_dev_texts   = dev_df_full[\"text\"].astype(str).tolist()\n",
    "\n",
    "y_train = train_df_small[label_cols].values.astype(\"float32\")\n",
    "y_dev   = dev_df_full[label_cols].values.astype(\"float32\")\n",
    "\n",
    "# ============================================================\n",
    "# 3. TOKENISASI SEDERHANA + VOCAB\n",
    "# ============================================================\n",
    "from collections import Counter\n",
    "\n",
    "def tokenize(text):\n",
    "    text = text.lower()\n",
    "    # ambil token alfanumerik sederhana\n",
    "    tokens = re.findall(r\"\\b\\w+\\b\", text)\n",
    "    return tokens\n",
    "\n",
    "# Bangun vocab dari TRAIN SAJA (jangan dari dev)\n",
    "counter = Counter()\n",
    "for txt in X_train_texts:\n",
    "    counter.update(tokenize(txt))\n",
    "\n",
    "MAX_VOCAB_SIZE = 30000  # bisa diubah\n",
    "most_common = counter.most_common(MAX_VOCAB_SIZE)\n",
    "\n",
    "# 0 = PAD, 1 = UNK\n",
    "word2idx = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "for i, (word, freq) in enumerate(most_common, start=2):\n",
    "    word2idx[word] = i\n",
    "\n",
    "idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "\n",
    "vocab_size = len(word2idx)\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "\n",
    "def numericalize(tokens, word2idx):\n",
    "    return [word2idx.get(tok, word2idx[\"<UNK>\"]) for tok in tokens]\n",
    "\n",
    "MAX_LEN = 50  # panjang sequence, bisa diubah\n",
    "\n",
    "def encode_texts(texts, word2idx, max_len):\n",
    "    all_ids = []\n",
    "    for txt in texts:\n",
    "        toks = tokenize(txt)\n",
    "        ids = numericalize(toks, word2idx)\n",
    "        # pad / truncate\n",
    "        if len(ids) < max_len:\n",
    "            ids = ids + [word2idx[\"<PAD>\"]] * (max_len - len(ids))\n",
    "        else:\n",
    "            ids = ids[:max_len]\n",
    "        all_ids.append(ids)\n",
    "    return np.array(all_ids, dtype=\"int64\")\n",
    "\n",
    "print(\"Encoding train texts...\")\n",
    "X_train_ids = encode_texts(X_train_texts, word2idx, MAX_LEN)\n",
    "print(\"Encoding dev texts...\")\n",
    "X_dev_ids   = encode_texts(X_dev_texts,   word2idx, MAX_LEN)\n",
    "\n",
    "# ============================================================\n",
    "# 4. DATASET & DATALOADER\n",
    "# ============================================================\n",
    "X_train_tensor = torch.tensor(X_train_ids, dtype=torch.long)\n",
    "y_train_tensor = torch.tensor(y_train,     dtype=torch.float32)\n",
    "\n",
    "X_dev_tensor   = torch.tensor(X_dev_ids,   dtype=torch.long)\n",
    "y_dev_tensor   = torch.tensor(y_dev,       dtype=torch.float32)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "dev_dataset   = TensorDataset(X_dev_tensor,   y_dev_tensor)\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "dev_loader   = DataLoader(dev_dataset,   batch_size=128, shuffle=False)\n",
    "\n",
    "print(\"Train batches:\", len(train_loader))\n",
    "print(\"Dev batches:\", len(dev_loader))\n",
    "\n",
    "# ============================================================\n",
    "# 5. MODEL TEXTCNN\n",
    "# ============================================================\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 embed_dim,\n",
    "                 num_labels,\n",
    "                 kernel_sizes=(3, 4, 5),\n",
    "                 num_filters=100,\n",
    "                 dropout=0.5,\n",
    "                 pad_idx=0):\n",
    "        super(TextCNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=embed_dim,\n",
    "            padding_idx=pad_idx\n",
    "        )\n",
    "\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(\n",
    "                in_channels=embed_dim,\n",
    "                out_channels=num_filters,\n",
    "                kernel_size=k\n",
    "            )\n",
    "            for k in kernel_sizes\n",
    "        ])\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(num_filters * len(kernel_sizes), num_labels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch, seq_len]\n",
    "        embedded = self.embedding(x)          # [B, L, E]\n",
    "        embedded = embedded.transpose(1, 2)   # [B, E, L]\n",
    "\n",
    "        conv_outs = [torch.relu(conv(embedded)) for conv in self.convs]\n",
    "        # conv_out: [B, num_filters, L_out]\n",
    "\n",
    "        pooled = [torch.max(co, dim=2)[0] for co in conv_outs]\n",
    "        # pooled: [B, num_filters]\n",
    "\n",
    "        cat = torch.cat(pooled, dim=1)        # [B, num_filters * len(kernel_sizes)]\n",
    "        cat = self.dropout(cat)\n",
    "        logits = self.fc(cat)                 # [B, num_labels]\n",
    "        return logits\n",
    "\n",
    "num_labels = len(label_cols)\n",
    "\n",
    "embed_dim   = 100\n",
    "kernel_sizes = (3, 4, 5)\n",
    "num_filters = 100\n",
    "dropout     = 0.5\n",
    "\n",
    "model = TextCNN(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=embed_dim,\n",
    "    num_labels=num_labels,\n",
    "    kernel_sizes=kernel_sizes,\n",
    "    num_filters=num_filters,\n",
    "    dropout=dropout,\n",
    "    pad_idx=word2idx[\"<PAD>\"]\n",
    ").to(device)\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "print(model)\n",
    "\n",
    "# ============================================================\n",
    "# 6. EVALUATION FUNCTION\n",
    "# ============================================================\n",
    "def evaluate(threshold=0.5):\n",
    "    model.eval()\n",
    "    all_logits = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for ids, labels in dev_loader:\n",
    "            ids    = ids.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            logits = model(ids)\n",
    "\n",
    "            all_logits.append(logits.cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "    all_logits = np.concatenate(all_logits, axis=0)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "    probs = 1 / (1 + np.exp(-all_logits))  # sigmoid\n",
    "    preds = (probs >= threshold).astype(int)\n",
    "\n",
    "    micro = f1_score(all_labels, preds, average=\"micro\")\n",
    "    macro = f1_score(all_labels, preds, average=\"macro\")\n",
    "\n",
    "    return micro, macro, probs, preds, all_labels\n",
    "\n",
    "# ============================================================\n",
    "# 7. TRAINING LOOP + SIMPAN BEST MODEL (BERDASARKAN MICRO-F1 DEV)\n",
    "# ============================================================\n",
    "epochs = 15\n",
    "\n",
    "best_micro = -1.0\n",
    "best_state = None\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{epochs}\")\n",
    "    for ids, labels in pbar:\n",
    "        ids    = ids.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(ids)\n",
    "        loss   = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * ids.size(0)\n",
    "        pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "    train_loss = total_loss / len(train_loader.dataset)\n",
    "    dev_micro, dev_macro, _, _, _ = evaluate(threshold=0.5)\n",
    "\n",
    "    print(f\"\\nEpoch {epoch} done | train_loss={train_loss:.4f} | \"\n",
    "          f\"dev_micro={dev_micro:.4f} | dev_macro={dev_macro:.4f}\")\n",
    "\n",
    "    if dev_micro > best_micro:\n",
    "        best_micro = dev_micro\n",
    "        best_state = model.state_dict().copy()\n",
    "        print(f\"  → New best model (dev Micro-F1={best_micro:.4f})\")\n",
    "\n",
    "# restore best model\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n",
    "    print(\"\\nBest model restored!\")\n",
    "\n",
    "# ============================================================\n",
    "# 8. CARI THRESHOLD GLOBAL TERBAIK DI FULL DEV\n",
    "# ============================================================\n",
    "print(\"\\nSearching best global threshold on FULL dev...\")\n",
    "\n",
    "best_t = 0.5\n",
    "best_t_micro = -1.0\n",
    "\n",
    "for t in np.arange(0.1, 0.9, 0.05):\n",
    "    micro_t, _, _, _, _ = evaluate(threshold=t)\n",
    "    print(f\"t={t:.2f} → Dev Micro-F1 = {micro_t:.4f}\")\n",
    "    if micro_t > best_t_micro:\n",
    "        best_t_micro = micro_t\n",
    "        best_t = t\n",
    "\n",
    "print(f\"\\nBEST threshold: {best_t:.2f}\")\n",
    "print(f\"BEST dev Micro-F1: {best_t_micro:.4f}\")\n",
    "\n",
    "# ============================================================\n",
    "# 9. FINAL EVAL + PER-LABEL F1 DI FULL DEV\n",
    "# ============================================================\n",
    "final_micro, final_macro, final_probs, final_preds, final_true = evaluate(threshold=best_t)\n",
    "\n",
    "print(\"\\n========================\")\n",
    "print(\" FINAL DEV PERFORMANCE  \")\n",
    "print(\"========================\")\n",
    "print(f\"Micro-F1 (t={best_t:.2f}): {final_micro:.2f}\")\n",
    "print(f\"Macro-F1 (t={best_t:.2f}): {final_macro:.2f}\\n\")\n",
    "\n",
    "for i, col in enumerate(label_cols):\n",
    "    f1 = f1_score(final_true[:, i], final_preds[:, i])\n",
    "    print(f\"{col.capitalize()} F1: {f1:.2f}\")\n",
    "\n",
    "# ============================================================\n",
    "# 10. SAVE dev_predictions_textcnn.csv\n",
    "# ============================================================\n",
    "output_df = dev_df_full.copy()\n",
    "for i, col in enumerate(label_cols):\n",
    "    output_df[col] = final_preds[:, i]\n",
    "\n",
    "output_df.to_csv(\"dev_predictions_textcnn.csv\", index=False)\n",
    "print(\"\\nSaved dev_predictions_textcnn.csv!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f0d279-3b24-40c6-9951-f77b931c46f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python for Language Modelling",
   "language": "python",
   "name": "lm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
